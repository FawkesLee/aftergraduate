{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在深度学习中，经常会使用EMA（指数移动平均，Exponential Movin Average,也叫做权重移动平均,Weighted Moving Average）这个方法对模型的参数做平均，以求提高测试指标并增加模型鲁棒。可以用来估计变量的局部均值，使得变量的更新与一段时间内的历史取值有关。\n",
    "# 什么是EMA？\n",
    "滑动平均可以看作是变量的过去一段时间取值的均值，相比对变量直接赋值而言，滑动平均得到的值在图像上更加平缓光滑，抖动性更小，不会因为某次的异常取值而使得滑动平均值波动很大。\n",
    "假定得到一个参数$\\theta$在不同epoch下的值:$[\\theta_1,\\theta_2,...,\\theta_t]$。当训练结束的$\\theta$的Moving Average 就是：$v_t=\\beta*v_{t-1}+(1-\\beta)*v_t$,$\\beta$是衰减率，用于控制模型更新的速度。Andrew Ng在Course 2 Improving Deep Neural Networks中讲到，t时刻变量v的滑动平均值大致等于过去$\\frac{1}{( 1 − \\beta )}$个时刻 $v$值的平均。\n",
    "![EMA](https://gitee.com/FawkesDoris/drawing-bed/raw/master/img/EMA.png)\n",
    "图一：不同$\\beta$值做EMA的效果对比（天气预报数据）\n",
    "当$\\beta$越大，滑动平均得到的值越和$v$的历史值相关。如果$\\beta=0.9$，则大致等于过去10个$v$值的平均;如果$\\beta=0.99$,则大致等于过去100个$v$值的平均。\n",
    "**滑动平均的好处: 占内存少，不需要保存过去10个或100个历史$v$值，就能估计均值。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TensorFlow实现\n",
    "TensorFlow 提供了 [tf.train.ExponentialMovingAverage](https://tensorflow.google.cn/api_docs/python/tf/train/ExponentialMovingAverage)来实现滑动平均。\n",
    "Example usage when creating a training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create variables.\n",
    "var0 = tf.Variable(...)\n",
    "var1 = tf.Variable(...)\n",
    "# ... use the variables to build a training model...\n",
    "...\n",
    "# Create an op that applies the optimizer.  This is what we usually\n",
    "# would use as a training op.\n",
    "opt_op = opt.minimize(my_loss, [var0, var1])\n",
    "\n",
    "# Create an ExponentialMovingAverage object\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
    "\n",
    "with tf.control_dependencies([opt_op]):\n",
    "    # Create the shadow variables, and add ops to maintain moving averages\n",
    "    # of var0 and var1. This also creates an op that will update the moving\n",
    "    # averages after each training step.  This is what we will use in place\n",
    "    # of the usual training op.\n",
    "    training_op = ema.apply([var0, var1])\n",
    "\n",
    "...train the model by running training_op..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch实现\n",
    "官方目前未提供EMA的实现，不过也并不是很复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "'''\n",
    "实现EMA\n",
    "'''\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "'''\n",
    "EMA使用示例\n",
    "'''\n",
    "# 初始化\n",
    "ema = EMA(model, 0.999)\n",
    "ema.register()\n",
    "\n",
    "# 训练过程中，更新完参数后，同步update shadow weights\n",
    "optimizer = Adam(...)\n",
    "def train():\n",
    "    optimizer.step()\n",
    "    ema.update()\n",
    "\n",
    "# eval前，apply shadow weights；eval之后，恢复原来模型的参数\n",
    "def evaluate():\n",
    "    ema.apply_shadow()\n",
    "    # evaluate\n",
    "    ema.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
